transformer
machine
learning
model
wikipedia
free
encyclopedia
jump
navigation
jump
search
machine
learning
algorithm
used
natural
language
processing
mw
parser
output
sidebar
width
em
float
right
clear
right
margin
em
em
em
background
f
f
fa
border
px
solid
aaa
padding
em
text
align
center
line
height
em
font
size
border
collapse
collapse
display
table
body
skin
minerva
mw
parser
output
sidebar
display
table
important
float
right
important
margin
em
em
em
important
mw
parser
output
sidebar
subgroup
width
margin
border
spacing
mw
parser
output
sidebar
left
float
left
clear
left
margin
em
em
em
mw
parser
output
sidebar
none
float
none
clear
margin
em
em
em
mw
parser
output
sidebar
outer
title
padding
em
em
font
size
line
height
em
font
weight
bold
mw
parser
output
sidebar
top
image
padding
em
mw
parser
output
sidebar
top
caption
mw
parser
output
sidebar
pretitle
top
image
mw
parser
output
sidebar
caption
padding
em
em
line
height
em
mw
parser
output
sidebar
pretitle
padding
em
em
line
height
em
mw
parser
output
sidebar
title
mw
parser
output
sidebar
title
pretitle
padding
em
em
font
size
line
height
em
mw
parser
output
sidebar
title
pretitle
padding
em
em
mw
parser
output
sidebar
image
padding
em
em
em
mw
parser
output
sidebar
heading
padding
em
em
mw
parser
output
sidebar
content
padding
em
em
mw
parser
output
sidebar
content
subgroup
padding
em
em
em
mw
parser
output
sidebar
mw
parser
output
sidebar
padding
em
em
font
weight
bold
mw
parser
output
sidebar
collapse
sidebar
mw
parser
output
sidebar
collapse
sidebar
border
top
px
solid
aaa
border
bottom
px
solid
aaa
mw
parser
output
sidebar
navbar
text
align
right
font
size
padding
em
em
mw
parser
output
sidebar
list
title
padding
em
text
align
left
font
weight
bold
line
height
em
font
size
mw
parser
output
sidebar
list
title
c
padding
em
text
align
center
margin
em
max
width
px
body
mediawiki
mw
parser
output
sidebar
width
important
clear
float
none
important
margin
left
important
margin
right
important
part
series
machine
learning
data
mining
problems
classification
clustering
regression
anomaly
detection
data
cleaning
automl
association
rules
reinforcement
learning
structured
prediction
feature
engineering
feature
learning
online
learning
semi
supervised
learning
unsupervised
learning
learning
rank
grammar
induction
supervised
learning
mw
parser
output
nobold
font
weight
normal
classification
regression
decision
trees
ensembles
bagging
boosting
random
forest
k
nn
linear
regression
naive
bayes
artificial
neural
networks
logistic
regression
perceptron
relevance
vector
machine
rvm
support
vector
machine
svm
clustering
birch
cure
hierarchical
k
means
expectation
maximization
em
dbscan
optics
mean
shift
dimensionality
reduction
factor
analysis
cca
ica
lda
nmf
pca
pgd
sne
structured
prediction
graphical
models
bayes
net
conditional
random
field
hidden
markov
anomaly
detection
k
nn
local
outlier
factor
artificial
neural
network
autoencoder
cognitive
computing
deep
learning
deepdream
multilayer
perceptron
rnn
lstm
gru
esn
restricted
boltzmann
machine
gan
som
convolutional
neural
network
u
net
transformer
vision
spiking
neural
network
memtransistor
electrochemical
ram
ecram
reinforcement
learning
q
learning
sarsa
temporal
difference
td
theory
kernel
machines
bias
variance
tradeoff
computational
learning
theory
empirical
risk
minimization
occam
learning
pac
learning
statistical
learning
vc
theory
machine
learning
venues
neurips
icml
ml
jmlr
arxiv
cs
lg
related
articles
glossary
artificial
intelligence
list
datasets
machine
learning
research
outline
machine
learning
mw
parser
output
navbar
display
inline
font
size
font
weight
normal
mw
parser
output
navbar
collapse
float
left
text
align
left
mw
parser
output
navbar
boxtext
word
spacing
mw
parser
output
navbar
ul
display
inline
block
white
space
nowrap
line
height
inherit
mw
parser
output
navbar
brackets
margin
right
em
content
mw
parser
output
navbar
brackets
margin
left
em
content
mw
parser
output
navbar
li
word
spacing
em
mw
parser
output
navbar
span
mw
parser
output
navbar
abbr
text
decoration
inherit
mw
parser
output
navbar
mini
abbr
font
variant
small
caps
border
bottom
none
text
decoration
none
cursor
inherit
mw
parser
output
navbar
ct
full
font
size
margin
em
mw
parser
output
navbar
ct
mini
font
size
margin
em
v
e
transformer
deep
learning
model
adopts
mechanism
self
attention
differentially
weighting
significance
part
input
data
used
primarily
fields
natural
language
processing
nlp
computer
vision
cv
like
recurrent
neural
networks
rnns
transformers
designed
handle
sequential
input
data
natural
language
tasks
translation
text
summarization
however
unlike
rnns
transformers
necessarily
process
data
order
rather
attention
mechanism
provides
context
position
input
sequence
example
input
data
natural
language
sentence
transformer
need
process
beginning
sentence
end
rather
identifies
context
confers
meaning
word
sentence
feature
allows
parallelization
rnns
therefore
reduces
training
times
transformers
introduced
team
google
brain
increasingly
model
choice
nlp
problems
replacing
rnn
models
long
short
term
memory
lstm
additional
training
parallelization
allows
training
larger
datasets
possible
led
development
pretrained
systems
bert
bidirectional
encoder
representations
transformers
gpt
generative
pre
trained
transformer
trained
large
language
datasets
wikipedia
corpus
common
crawl
fine
tuned
specific
tasks
mw
parser
output
toclimit
toclevel
ul
mw
parser
output
toclimit
toclevel
ul
mw
parser
output
toclimit
toclevel
ul
mw
parser
output
toclimit
toclevel
ul
mw
parser
output
toclimit
toclevel
ul
mw
parser
output
toclimit
toclevel
ul
display
none
contents
background
sequential
processing
attention
architecture
encoder
decoder
architecture
scaled
dot
product
attention
multi
head
attention
encoder
decoder
alternatives
training
applications
implementations
see
also
references
reading
external
links
background
edit
transformers
state
art
nlp
systems
relied
gated
rnns
lstm
gated
recurrent
units
grus
added
attention
mechanisms
transformers
built
attention
technologies
without
using
rnn
structure
highlighting
fact
attention
mechanisms
alone
match
performance
rnns
attention
sequential
processing
edit
gated
rnns
process
tokens
sequentially
maintaining
state
vector
contains
representation
data
seen
every
token
process
n
textstyle
n
th
token
model
combines
state
representing
sentence
token
n
textstyle
n
information
new
token
create
new
state
representing
sentence
token
n
textstyle
n
theoretically
information
one
token
propagate
arbitrarily
far
sequence
every
point
state
continues
encode
contextual
information
token
practice
mechanism
flawed
vanishing
gradient
problem
leaves
model
state
end
long
sentence
without
precise
extractable
information
preceding
tokens
dependency
token
computations
results
previous
token
computations
also
makes
hard
parallelize
computation
modern
deep
learning
hardware
make
training
rnns
inefficient
attention
edit
problems
addressed
attention
mechanisms
attention
mechanisms
let
model
draw
state
preceding
point
along
sequence
attention
layer
access
previous
states
weigh
according
learned
measure
relevancy
providing
relevant
information
far
away
tokens
clear
example
value
attention
language
translation
context
essential
assign
meaning
word
sentence
english
french
translation
system
first
word
french
output
probably
depends
heavily
first
words
english
input
however
classic
lstm
model
order
produce
first
word
french
output
model
given
state
vector
last
english
word
theoretically
vector
encode
information
whole
english
sentence
giving
model
necessary
knowledge
practice
information
often
poorly
preserved
lstm
attention
mechanism
added
address
problem
decoder
given
access
state
vectors
every
english
input
word
last
learn
attention
weights
dictate
much
attend
english
input
state
vector
added
rnns
attention
mechanisms
increase
performance
development
transformer
architecture
revealed
attention
mechanisms
powerful
sequential
recurrent
processing
data
necessary
achieve
quality
gains
rnns
attention
transformers
use
attention
mechanism
without
rnn
processing
tokens
time
calculating
attention
weights
successive
layers
since
attention
mechanism
uses
information
tokens
lower
layers
computed
tokens
parallel
leads
improved
training
speed
architecture
edit
encoder
decoder
architecture
edit
like
earlier
seq
seq
models
original
transformer
model
used
encoder
decoder
architecture
encoder
consists
encoding
layers
process
input
iteratively
one
layer
another
decoder
consists
decoding
layers
thing
encoder
output
function
encoder
layer
generate
encodings
contain
information
parts
inputs
relevant
passes
encodings
next
encoder
layer
inputs
decoder
layer
opposite
taking
encodings
using
incorporated
contextual
information
generate
output
sequence
achieve
encoder
decoder
layer
makes
use
attention
mechanism
input
attention
weighs
relevance
every
input
draws
produce
output
decoder
layer
additional
attention
mechanism
draws
information
outputs
previous
decoders
decoder
layer
draws
information
encodings
encoder
decoder
layers
feed
forward
neural
network
additional
processing
outputs
contain
residual
connections
layer
normalization
steps
scaled
dot
product
attention
edit
transformer
building
blocks
scaled
dot
product
attention
units
sentence
passed
transformer
model
attention
weights
calculated
every
token
simultaneously
attention
unit
produces
embeddings
every
token
context
contain
information
token
along
weighted
combination
relevant
tokens
weighted
attention
weight
attention
unit
transformer
model
learns
three
weight
matrices
query
weights
w
q
displaystyle
w
q
key
weights
w
k
displaystyle
w
k
value
weights
w
v
displaystyle
w
v
token
displaystyle
input
word
embedding
x
displaystyle
x
multiplied
three
weight
matrices
produce
query
vector
q
x
w
q
displaystyle
q
x
w
q
key
vector
k
x
w
k
displaystyle
k
x
w
k
value
vector
v
x
w
v
displaystyle
v
x
w
v
attention
weights
calculated
using
query
key
vectors
attention
weight
j
displaystyle
ij
token
displaystyle
token
j
displaystyle
j
dot
product
q
displaystyle
q
k
j
displaystyle
k
j
attention
weights
divided
square
root
dimension
key
vectors
k
displaystyle
sqrt
k
stabilizes
gradients
training
passed
softmax
normalizes
weights
fact
w
q
displaystyle
w
q
w
k
displaystyle
w
k
different
matrices
allows
attention
non
symmetric
token
displaystyle
attends
token
j
displaystyle
j
e
q
x
c
k
j
displaystyle
q
cdot
k
j
large
necessarily
mean
token
j
displaystyle
j
attend
token
displaystyle
e
q
j
x
c
k
displaystyle
q
j
cdot
k
could
small
output
attention
unit
token
displaystyle
weighted
sum
value
vectors
tokens
weighted
j
displaystyle
ij
attention
token
displaystyle
token
attention
calculation
tokens
expressed
one
large
matrix
calculation
using
softmax
function
useful
training
due
computational
matrix
operation
optimizations
quickly
compute
matrix
operations
matrices
q
displaystyle
q
k
displaystyle
k
v
displaystyle
v
defined
matrices
displaystyle
th
rows
vectors
q
displaystyle
q
k
displaystyle
k
v
displaystyle
v
respectively
attention
q
k
v
softmax
q
k
k
v
displaystyle
begin
aligned
text
attention
q
k
v
text
softmax
left
frac
qk
mathrm
sqrt
k
right
v
end
aligned
multi
head
attention
edit
one
set
w
q
w
k
w
v
displaystyle
left
w
q
w
k
w
v
right
matrices
called
attention
head
layer
transformer
model
multiple
attention
heads
attention
head
attends
tokens
relevant
token
multiple
attention
heads
model
different
definitions
relevance
addition
influence
field
representing
relevance
become
progressively
dilated
successive
layers
many
transformer
attention
heads
encode
relevance
relations
meaningful
humans
example
attention
heads
attend
mostly
next
word
others
mainly
attend
verbs
direct
objects
computations
attention
head
performed
parallel
allows
fast
processing
outputs
attention
layer
concatenated
pass
feed
forward
neural
network
layers
encoder
edit
encoder
consists
two
major
components
self
attention
mechanism
feed
forward
neural
network
self
attention
mechanism
accepts
input
encodings
previous
encoder
weighs
relevance
generate
output
encodings
feed
forward
neural
network
processes
output
encoding
individually
output
encodings
passed
next
encoder
input
well
decoders
first
encoder
takes
positional
information
embeddings
input
sequence
input
rather
encodings
positional
information
necessary
transformer
make
use
order
sequence
part
transformer
makes
use
decoder
edit
decoder
consists
three
major
components
self
attention
mechanism
attention
mechanism
encodings
feed
forward
neural
network
decoder
functions
similar
fashion
encoder
additional
attention
mechanism
inserted
instead
draws
relevant
information
encodings
generated
encoders
like
first
encoder
first
decoder
takes
positional
information
embeddings
output
sequence
input
rather
encodings
transformer
must
use
current
future
output
predict
output
output
sequence
must
partially
masked
prevent
reverse
information
flow
last
decoder
followed
final
linear
transformation
softmax
layer
produce
output
probabilities
vocabulary
alternatives
edit
training
transformer
based
architectures
expensive
especially
long
inputs
alternative
architectures
include
reformer
reduces
computational
load
n
displaystyle
n
n
ln
n
displaystyle
n
ln
n
models
like
etc
bigbird
reduce
n
displaystyle
n
n
displaystyle
n
length
sequence
done
using
locality
sensitive
hashing
reversible
layers
benchmark
comparing
transformer
architectures
introduced
late
training
edit
transformers
typically
undergo
semi
supervised
learning
involving
unsupervised
pretraining
followed
supervised
fine
tuning
pretraining
typically
done
larger
dataset
fine
tuning
due
limited
availability
labeled
training
data
tasks
pretraining
fine
tuning
commonly
include
language
modeling
next
sentence
prediction
question
answering
reading
comprehension
sentiment
analysis
paraphrasing
applications
edit
transformer
great
success
natural
language
processing
nlp
example
tasks
machine
translation
time
series
prediction
many
pretrained
models
gpt
gpt
bert
xlnet
roberta
demonstrate
ability
transformers
perform
wide
variety
nlp
related
tasks
potential
find
real
world
applications
may
include
machine
translation
document
summarization
document
generation
named
entity
recognition
ner
biological
sequence
analysis
video
understanding
shown
transformer
architecture
specifically
gpt
could
tuned
play
chess
transformers
applied
image
processing
results
competitive
convolutional
neural
networks
implementations
edit
transformer
model
implemented
standard
deep
learning
frameworks
tensorflow
pytorch
transformers
library
produced
hugging
face
supplies
transformer
based
architectures
pretrained
models
see
also
edit
perceiver
gpt
wu
dao
vision
transformers
references
edit
mw
parser
output
reflist
font
size
margin
bottom
em
list
style
type
decimal
mw
parser
output
reflist
references
font
size
margin
bottom
list
style
type
inherit
mw
parser
output
reflist
columns
column
width
em
mw
parser
output
reflist
columns
column
width
em
mw
parser
output
reflist
columns
margin
top
em
mw
parser
output
reflist
columns
ol
margin
top
mw
parser
output
reflist
columns
li
page
break
inside
avoid
break
inside
avoid
column
mw
parser
output
reflist
upper
alpha
list
style
type
upper
alpha
mw
parser
output
reflist
upper
roman
list
style
type
upper
roman
mw
parser
output
reflist
lower
alpha
list
style
type
lower
alpha
mw
parser
output
reflist
lower
greek
list
style
type
lower
greek
mw
parser
output
reflist
lower
roman
list
style
type
lower
roman
b
c
e
f
mw
parser
output
cite
citation
font
style
inherit
word
wrap
break
word
mw
parser
output
citation
q
quotes
mw
parser
output
citation
target
background
color
rgba
mw
parser
output
id
lock
free
mw
parser
output
citation
cs
lock
free
background
linear
gradient
transparent
transparent
url
upload
wikimedia
org
wikipedia
commons
lock
green
svg
right
em
center
px
repeat
mw
parser
output
id
lock
limited
mw
parser
output
id
lock
registration
mw
parser
output
citation
cs
lock
limited
mw
parser
output
citation
cs
lock
registration
background
linear
gradient
transparent
transparent
url
upload
wikimedia
org
wikipedia
commons
lock
gray
alt
svg
right
em
center
px
repeat
mw
parser
output
id
lock
subscription
mw
parser
output
citation
cs
lock
subscription
background
linear
gradient
transparent
transparent
url
upload
wikimedia
org
wikipedia
commons
aa
lock
red
alt
svg
right
em
center
px
repeat
mw
parser
output
cs
ws
icon
background
linear
gradient
transparent
transparent
url
upload
wikimedia
org
wikipedia
commons
c
wikisource
logo
svg
right
em
center
px
repeat
mw
parser
output
cs
code
color
inherit
background
inherit
border
none
padding
inherit
mw
parser
output
cs
hidden
error
display
none
color
mw
parser
output
cs
visible
error
color
mw
parser
output
cs
maint
display
none
color
margin
left
em
mw
parser
output
cs
format
font
size
mw
parser
output
cs
kern
left
padding
left
em
mw
parser
output
cs
kern
right
padding
right
em
mw
parser
output
citation
mw
selflink
font
weight
inherit
vaswani
ashish
shazeer
noam
parmar
niki
uszkoreit
jakob
jones
llion
gomez
aidan
n
kaiser
lukasz
polosukhin
illia
attention
need
arxiv
cs
cl
cheng
transformer
cv
transformer
cv
towards
data
science
b
wolf
thomas
debut
lysandre
sanh
victor
chaumond
julien
delangue
clement
moi
anthony
cistac
pierric
rault
tim
louf
remi
funtowicz
morgan
davison
joe
shleifer
sam
von
platen
patrick
clara
jernite
yacine
plu
julien
xu
canwen
le
scao
teven
gugger
sylvain
drame
mariama
lhoest
quentin
rush
alexander
transformers
state
art
natural
language
processing
proceedings
conference
empirical
methods
natural
language
processing
system
demonstrations
pp
doi
v
emnlp
demos
cid
b
c
open
sourcing
bert
state
art
pre
training
natural
language
processing
google
ai
blog
retrieved
b
c
better
language
models
implications
openai
retrieved
sequence
modeling
neural
networks
part
attention
models
indico
retrieved
b
c
alammar
jay
illustrated
transformer
jalammar
github
io
retrieved
clark
kevin
khandelwal
urvashi
levy
omer
manning
christopher
august
bert
look
analysis
bert
attention
proceedings
acl
workshop
blackboxnlp
analyzing
interpreting
neural
networks
nlp
florence
italy
association
computational
linguistics
doi
v
w
kitaev
nikita
kaiser
ukasz
levskaya
anselm
reformer
efficient
transformer
arxiv
cs
lg
constructing
transformers
longer
sequences
sparse
attention
methods
google
ai
blog
retrieved
tasks
long
sequences
chatbot
coursera
reformer
efficient
transformer
google
ai
blog
retrieved
tay
yi
dehghani
mostafa
abnar
samira
shen
yikang
bahri
dara
pham
philip
rao
jinfeng
yang
liu
ruder
sebastian
metzler
donald
long
range
arena
benchmark
efficient
transformers
arxiv
cs
lg
b
wang
alex
singh
amanpreet
michael
julian
hill
felix
levy
omer
bowman
samuel
glue
multi
task
benchmark
analysis
platform
natural
language
understanding
proceedings
emnlp
workshop
blackboxnlp
analyzing
interpreting
neural
networks
nlp
stroudsburg
pa
usa
association
computational
linguistics
arxiv
doi
v
w
cid
allard
maxime
transformer
medium
retrieved
yang
zhilin
dai
zihang
yang
yiming
carbonell
jaime
salakhutdinov
ruslan
le
quoc
v
xlnet
generalized
autoregressive
pretraining
language
understanding
oclc
cite
book
cs
maint
multiple
names
authors
list
link
monsters
data
applications
artificial
neural
networks
natural
language
processing
medium
retrieved
rives
alexander
goyal
siddharth
meier
joshua
guo
demi
ott
myle
zitnick
c
lawrence
jerry
fergus
rob
biological
structure
function
emerge
scaling
unsupervised
learning
million
protein
sequences
biorxiv
nambiar
ananthan
heflin
maeve
liu
simon
maslov
sergei
hopkins
mark
ritz
anna
transforming
language
life
transformer
neural
networks
protein
prediction
tasks
doi
cid
cite
journal
cite
journal
requires
journal
help
rao
roshan
bhattacharya
nicholas
thomas
neil
duan
yan
chen
xi
canny
john
abbeel
pieter
song
yun
evaluating
protein
transfer
learning
tape
biorxiv
bertasias
wang
torresani
space
time
attention
need
video
understanding
arxiv
cs
cv
noever
david
ciolino
matt
kalin
josh
chess
transformer
mastering
play
using
generative
language
models
arxiv
cs
ai
dosovitskiy
alexey
beyer
lucas
kolesnikov
alexander
weissenborn
dirk
zhai
xiaohua
unterthiner
thomas
dehghani
mostafa
minderer
matthias
heigold
georg
gelly
sylvain
uszkoreit
jakob
houlsby
neil
image
worth
x
words
transformers
image
recognition
scale
pdf
arxiv
cite
web
cs
maint
url
status
link
touvron
hugo
cord
matthieu
douze
matthijs
massa
francisco
sablayrolles
alexandre
j
gou
herv
training
data
efficient
image
transformers
distillation
attention
pdf
arxiv
cite
web
cs
maint
url
status
link
reading
edit
hubert
ramsauer
et
al
hopfield
networks
need
preprint
submitted
iclr
arxiv
see
also
authors
blog
discussion
effect
transformer
layer
equivalent
hopfield
update
bringing
input
closer
one
fixed
points
representable
patterns
continuous
valued
hopfield
network
external
links
edit
alexander
rush
annotated
transformer
harvard
nlp
group
april
mw
parser
output
navbox
box
sizing
border
box
border
px
solid
b
width
clear
font
size
text
align
center
padding
px
margin
em
auto
mw
parser
output
navbox
navbox
margin
top
mw
parser
output
navbox
navbox
mw
parser
output
navbox
navbox
styles
navbox
margin
top
px
mw
parser
output
navbox
inner
mw
parser
output
navbox
subgroup
width
mw
parser
output
navbox
group
mw
parser
output
navbox
title
mw
parser
output
navbox
abovebelow
padding
em
em
line
height
em
text
align
center
mw
parser
output
navbox
group
white
space
nowrap
text
align
right
mw
parser
output
navbox
mw
parser
output
navbox
subgroup
background
color
fdfdfd
mw
parser
output
navbox
list
line
height
em
border
color
fdfdfd
mw
parser
output
navbox
list
group
text
align
left
border
left
width
px
border
left
style
solid
mw
parser
output
tr
tr
navbox
abovebelow
mw
parser
output
tr
tr
navbox
group
mw
parser
output
tr
tr
navbox
image
mw
parser
output
tr
tr
navbox
list
border
top
px
solid
fdfdfd
mw
parser
output
navbox
title
background
color
ccf
mw
parser
output
navbox
abovebelow
mw
parser
output
navbox
group
mw
parser
output
navbox
subgroup
navbox
title
background
color
ddf
mw
parser
output
navbox
subgroup
navbox
group
mw
parser
output
navbox
subgroup
navbox
abovebelow
background
color
e
e
ff
mw
parser
output
navbox
even
background
color
f
f
f
mw
parser
output
navbox
odd
background
color
transparent
mw
parser
output
navbox
hlist
td
dl
mw
parser
output
navbox
hlist
td
ol
mw
parser
output
navbox
hlist
td
ul
mw
parser
output
navbox
td
hlist
dl
mw
parser
output
navbox
td
hlist
ol
mw
parser
output
navbox
td
hlist
ul
padding
em
mw
parser
output
navbox
navbar
display
block
font
size
mw
parser
output
navbox
title
navbar
float
left
text
align
left
margin
right
em
v
e
differentiable
computing
general
differentiable
programming
neural
turing
machine
differentiable
neural
computer
automatic
differentiation
neuromorphic
engineering
cable
theory
pattern
recognition
computational
learning
theory
tensor
calculus
concepts
gradient
descent
sgd
clustering
regression
overfitting
adversary
attention
convolution
loss
functions
backpropagation
normalization
activation
softmax
sigmoid
rectifier
regularization
datasets
augmentation
programming
languages
python
julia
application
machine
learning
artificial
neural
network
deep
learning
scientific
computing
artificial
intelligence
hardware
ipu
tpu
vpu
memristor
spinnaker
software
library
tensorflow
pytorch
keras
theano
implementation
audio
visual
alexnet
wavenet
human
image
synthesis
hwr
ocr
speech
synthesis
speech
recognition
facial
recognition
alphafold
dall
e
verbal
word
vec
transformer
bert
nmt
project
debater
watson
gpt
gpt
decisional
alphago
alphazero
q
learning
sarsa
openai
five
self
driving
car
muzero
action
selection
robot
control
people
alex
graves
ian
goodfellow
yoshua
bengio
geoffrey
hinton
yann
lecun
andrew
ng
demis
hassabis
david
silver
fei
fei
li
organizations
deepmind
openai
mit
csail
mila
google
brain
fair
portals
computer
programming
technology
category
artificial
neural
networks
machine
learning
newpp
limit
report
parsed
mw
cached
time
cache
expiry
reduced
expiry
false
complications
vary
revision
sha
cpu
time
usage
seconds
real
time
usage
seconds
preprocessor
visited
node
count
post
expand
include
size
bytes
template
argument
size
bytes
highest
expansion
depth
expensive
parser
function
count
unstrip
recursion
depth
unstrip
post
expand
size
bytes
lua
time
usage
seconds
lua
memory
usage
bytes
number
wikibase
entities
loaded
transclusion
expansion
time
report
ms
calls
template
total
template
reflist
template
cite
arxiv
template
machine
learning
template
sidebar
collapsible
lists
template
cite
web
template
short
description
template
differentiable
computing
template
navbox
template
pagetype
saved
parser
cache
key
enwiki
pcache
idhash
canonical
timestamp
revision
id
serialized
json
retrieved
oldid
categories
artificial
neural
networks
hidden
categories
cs
maint
multiple
names
authors
list
cs
errors
missing
periodical
cs
maint
url
status
articles
short
description
short
description
different
wikidata
navigation
menu
